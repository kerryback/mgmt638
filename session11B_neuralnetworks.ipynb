{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 15: Neural Networks\n",
    "\n",
    "Author: Kerry Back, Rice University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Neural networks are powerful machine learning models inspired by how neurons work in the brain. They learn to recognize patterns by adjusting internal parameters (weights and biases).\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "- Neurons: mathematical functions that transform inputs\n",
    "- Layers: groups of neurons (input, hidden, output)\n",
    "- Activation functions: non-linear transformations (sigmoid, ReLU)\n",
    "- Training: optimizing weights/biases to minimize prediction error"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Multi-layer Perceptrons\n\n* A multi-layer perceptron (MLP) consists of \"neurons\" arranged in layers.\n* A neuron is a mathematical function. It takes inputs $x_1, \\ldots, x_n$, calculates a function $y=f(x_1, \\ldots, x_n)$ and passes $y$ to the neurons in the next level.\n  * The inputs in the first layer are the features.\n  * The inputs in successive layers are the calculations from the prior level. \n* The last layer is a single neuron (or multiple neurons for multi-class classification) that produces the output.\n* \"Input layer\" doesn't do anything. \"Output layer\" is last layer. Others are called \"hidden layers.\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Rectified Linear Units\n\n- The usual function for the neurons (except in the last layer) is \n\n$$ y = \\max(0,b+w_1x_1 + \\cdots + w_nx_n)$$\n\n- Parameters $b$ (called bias) and $w_1, \\ldots w_n$ (called weights) are different for different neurons. \n- This function is called a rectified linear unit (ReLU).  \n- Analogous to neurons firing in brain:\n  - $y>0$ only when $\\sum w_ix_i$ is large enough. \n  - A neuron fires when it is sufficiently stimulated by signals from other neurons.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Output Neuron\n\n- The output doesn't have a truncation.\n- For regression problems, it is linear:\n\n$$z = b+w_1y_1 + \\cdots + w_ny_n$$ \n\n- For classification, there is a linear function for each class and predicted probabilities are (called softmax):\n$$ \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}$$",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Task: Digit Classification\n",
    "\n",
    "Classify hand-written numbers (digits 0 through 9).\n",
    "\n",
    "The original handwritten digits were pixelated into 64 pixels (8 x 8). Each observation consists of 65 numbers: the true digit and the darkness values of each of the 64 pixels. Darkness values range from 0 (white) to 16 (black)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore the Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digits Dataset:\n",
      "  Total samples: 1,797\n",
      "  Features per sample: 64 (8×8 pixels)\n",
      "  Classes: 10 (digits 0-9)\n",
      "  Feature range: [0.0, 16.0]\n"
     ]
    }
   ],
   "source": [
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.data  # 8x8 images flattened to 64 features\n",
    "y = digits.target  # Digit labels 0-9\n",
    "\n",
    "print(f\"Digits Dataset:\")\n",
    "print(f\"  Total samples: {X.shape[0]:,}\")\n",
    "print(f\"  Features per sample: {X.shape[1]} (8×8 pixels)\")\n",
    "print(f\"  Classes: {len(np.unique(y))} (digits 0-9)\")\n",
    "print(f\"  Feature range: [{X.min():.1f}, {X.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sample Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 10, figsize=(15, 4))\n",
    "\n",
    "for digit in range(10):\n",
    "    idx = np.where(y == digit)[0][0]\n",
    "    axes[0, digit].imshow(X[idx].reshape(8, 8), cmap='gray_r')\n",
    "    axes[0, digit].axis('off')\n",
    "\n",
    "    idx2 = np.where(y == digit)[0][1]\n",
    "    axes[1, digit].imshow(X[idx2].reshape(8, 8), cmap='gray_r')\n",
    "    axes[1, digit].axis('off')\n",
    "\n",
    "plt.suptitle('Two Examples Each of 0-9', fontsize=20, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Understanding Neural Networks with Digits\n\nWe'll explore how neural networks learn to classify handwritten digits. \n\nEach digit image has:\n- 64 inputs (pixel values from 0-16)\n- 10 possible outputs (digits 0-9)\n\nA simple neural network with one hidden layer can learn the complex patterns that distinguish different digits."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prepare Data for Neural Network"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Normalize the pixel values to [0, 1] range\nX_normalized = X / 16.0\n\n# Split into training and test sets (80/20 split)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_normalized, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Training set: {X_train.shape[0]:,} samples\")\nprint(f\"Test set: {X_test.shape[0]:,} samples\")\nprint(f\"Features: {X_train.shape[1]} (normalized pixel values)\")\nprint(f\"Classes: {len(np.unique(y))} digits (0-9)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Train a Simple Neural Network"
  },
  {
   "cell_type": "code",
   "source": "## Confusion Matrices",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Get predictions for both train and test sets\ny_train_pred = nn_model.predict(X_train)\ny_test_pred = nn_model.predict(X_test)\n\n# Create confusion matrices\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Training set confusion matrix\ncm_train = confusion_matrix(y_train, y_train_pred)\ndisp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=range(10))\ndisp_train.plot(ax=axes[0], cmap='Blues', colorbar=False)\naxes[0].set_title(f'Training Set Confusion Matrix\\nAccuracy: {train_accuracy:.2%}', fontsize=12)\n\n# Test set confusion matrix\ncm_test = confusion_matrix(y_test, y_test_pred)\ndisp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=range(10))\ndisp_test.plot(ax=axes[1], cmap='Blues', colorbar=False)\naxes[1].set_title(f'Test Set Confusion Matrix\\nAccuracy: {test_accuracy:.2%}', fontsize=12)\n\nplt.tight_layout()\nplt.show()\n\n# Show some statistics\nprint(\"Classification errors by digit (Test Set):\")\nfor digit in range(10):\n    mask = y_test == digit\n    digit_accuracy = (y_test_pred[mask] == digit).mean()\n    print(f\"  Digit {digit}: {digit_accuracy:.1%} accuracy\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.neural_network import MLPClassifier\n\n# Create a neural network with one hidden layer of 50 neurons\nnn_model = MLPClassifier(\n    hidden_layer_sizes=(50,),\n    activation='relu',\n    max_iter=500,\n    random_state=42,\n    verbose=False\n)\n\n# Train the model\nnn_model.fit(X_train, y_train)\n\n# Evaluate on both sets\ntrain_accuracy = nn_model.score(X_train, y_train)\ntest_accuracy = nn_model.score(X_test, y_test)\n\nprint(f\"Neural Network Performance:\")\nprint(f\"  Training accuracy: {train_accuracy:.2%}\")\nprint(f\"  Test accuracy: {test_accuracy:.2%}\")\nprint(f\"  Number of iterations: {nn_model.n_iter_}\")\nprint(f\"\\nNetwork Architecture:\")\nprint(f\"  Input layer: 64 neurons (one per pixel)\")\nprint(f\"  Hidden layer: 50 neurons with ReLU activation\")\nprint(f\"  Output layer: 10 neurons (one per digit)\")"
  },
  {
   "cell_type": "code",
   "source": "## Visualize Predictions",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Show some test examples with predictions\ny_pred = nn_model.predict(X_test)\n\nfig, axes = plt.subplots(2, 10, figsize=(15, 4))\nfor i in range(20):\n    row = i // 10\n    col = i % 10\n    axes[row, col].imshow(X_test[i].reshape(8, 8), cmap='gray_r')\n    axes[row, col].axis('off')\n    \n    # Color code: green for correct, red for incorrect\n    color = 'green' if y_pred[i] == y_test.iloc[i] else 'red'\n    axes[row, col].set_title(f'Pred: {y_pred[i]}', color=color, fontsize=10)\n\nplt.suptitle('Sample Predictions (Green=Correct, Red=Incorrect)', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## How Neurons Work\n\nA neuron calculates a linear function then applies activation:\n\n**Linear Calculation:** z = b + w1×x1 + w2×x2 + ... + w64×x64\n\n- b = bias (intercept)\n- w1, w2, ..., w64 = weights (one per pixel)\n\n**Activation Functions:**\n\n- ReLU: h = max(0, z) outputs values >= 0 (used in hidden layer)\n- Softmax: converts outputs to probabilities for 10 classes\n\nThe parameters (biases and weights) are learned from data during training."
  },
  {
   "cell_type": "code",
   "source": "## Network Complexity Analysis\n\nWe'll train multiple neural networks with increasing complexity and compare their performance on training and test data. This helps us understand:\n- How model capacity affects performance\n- Whether more complex models lead to overfitting\n- The trade-off between training and test accuracy",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## Train Networks of Varying Complexity",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define different network architectures (increasing complexity)\nnetwork_configs = [\n    (10,),           # Small: 10 neurons\n    (25,),           # Medium-small: 25 neurons\n    (50,),           # Medium: 50 neurons\n    (100,),          # Large: 100 neurons\n    (25, 10),        # Two layers: 25 -> 10\n    (50, 25),        # Two layers: 50 -> 25\n    (100, 50),       # Two layers: 100 -> 50\n    (50, 25, 10),    # Three layers: 50 -> 25 -> 10\n    (100, 50, 25),   # Three layers: 100 -> 50 -> 25\n]\n\n# Store results\nresults = []\ntrained_models = {}\n\nprint(\"Training networks of increasing complexity...\\n\")\n\nfor i, config in enumerate(network_configs):\n    # Create and train model\n    model = MLPClassifier(\n        hidden_layer_sizes=config,\n        activation='relu',\n        max_iter=500,\n        random_state=42,\n        verbose=False\n    )\n    \n    model.fit(X_train, y_train)\n    \n    # Calculate accuracies\n    train_acc = model.score(X_train, y_train)\n    test_acc = model.score(X_test, y_test)\n    \n    # Calculate number of parameters\n    n_params = sum(w.size for w in model.coefs_) + sum(b.size for b in model.intercepts_)\n    \n    # Store results\n    results.append({\n        'config': config,\n        'config_str': str(config),\n        'train_accuracy': train_acc,\n        'test_accuracy': test_acc,\n        'n_params': n_params,\n        'n_layers': len(config),\n        'model_idx': i\n    })\n    \n    trained_models[i] = model\n    \n    print(f\"{i+1}. Architecture {config}:\")\n    print(f\"   Parameters: {n_params:,}\")\n    print(f\"   Train accuracy: {train_acc:.2%}\")\n    print(f\"   Test accuracy: {test_acc:.2%}\")\n    print()\n\nresults_df = pd.DataFrame(results)\nprint(\"\\nSummary:\")\nprint(results_df[['config_str', 'n_params', 'train_accuracy', 'test_accuracy']].to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## Plot Accuracy vs. Model Complexity",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plot 1: Accuracy vs. Number of Parameters\naxes[0].plot(results_df['n_params'], results_df['train_accuracy'], \n             'o-', label='Training', linewidth=2, markersize=8)\naxes[0].plot(results_df['n_params'], results_df['test_accuracy'], \n             's-', label='Test', linewidth=2, markersize=8)\naxes[0].set_xlabel('Number of Parameters', fontsize=12)\naxes[0].set_ylabel('Accuracy', fontsize=12)\naxes[0].set_title('Accuracy vs. Model Size', fontsize=14)\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\n\n# Plot 2: Accuracy by Model Index\nx_pos = range(len(results_df))\nwidth = 0.35\naxes[1].bar([x - width/2 for x in x_pos], results_df['train_accuracy'], \n            width, label='Training', alpha=0.8)\naxes[1].bar([x + width/2 for x in x_pos], results_df['test_accuracy'], \n            width, label='Test', alpha=0.8)\naxes[1].set_xlabel('Model Configuration', fontsize=12)\naxes[1].set_ylabel('Accuracy', fontsize=12)\naxes[1].set_title('Accuracy by Network Architecture', fontsize=14)\naxes[1].set_xticks(x_pos)\naxes[1].set_xticklabels([f'M{i+1}' for i in x_pos], rotation=0)\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n# Print best models\nprint(\"\\nBest Models:\")\nbest_test_idx = results_df['test_accuracy'].idxmax()\nbest_train_idx = results_df['train_accuracy'].idxmax()\n\nprint(f\"\\nBest Test Accuracy:\")\nprint(f\"  Architecture: {results_df.loc[best_test_idx, 'config']}\")\nprint(f\"  Test accuracy: {results_df.loc[best_test_idx, 'test_accuracy']:.2%}\")\nprint(f\"  Train accuracy: {results_df.loc[best_test_idx, 'train_accuracy']:.2%}\")\nprint(f\"  Parameters: {results_df.loc[best_test_idx, 'n_params']:,}\")\n\nprint(f\"\\nBest Train Accuracy:\")\nprint(f\"  Architecture: {results_df.loc[best_train_idx, 'config']}\")\nprint(f\"  Train accuracy: {results_df.loc[best_train_idx, 'train_accuracy']:.2%}\")\nprint(f\"  Test accuracy: {results_df.loc[best_train_idx, 'test_accuracy']:.2%}\")\nprint(f\"  Parameters: {results_df.loc[best_train_idx, 'n_params']:,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "## Confusion Matrices for Selected Models",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Select three models to compare: small, medium, large\nselected_models = [0, 2, 6]  # (10,), (50,), (100, 50)\nmodel_names = ['Small (10)', 'Medium (50)', 'Large (100, 50)']\n\nfig, axes = plt.subplots(3, 2, figsize=(12, 16))\n\nfor idx, (model_idx, name) in enumerate(zip(selected_models, model_names)):\n    model = trained_models[model_idx]\n    config = results_df.loc[model_idx, 'config']\n    \n    # Get predictions\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    \n    # Training confusion matrix\n    cm_train = confusion_matrix(y_train, y_train_pred)\n    disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=range(10))\n    disp_train.plot(ax=axes[idx, 0], cmap='Blues', colorbar=False)\n    train_acc = results_df.loc[model_idx, 'train_accuracy']\n    axes[idx, 0].set_title(f'{name}: Training\\n{config} - Accuracy: {train_acc:.2%}', fontsize=11)\n    \n    # Test confusion matrix\n    cm_test = confusion_matrix(y_test, y_test_pred)\n    disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=range(10))\n    disp_test.plot(ax=axes[idx, 1], cmap='Blues', colorbar=False)\n    test_acc = results_df.loc[model_idx, 'test_accuracy']\n    axes[idx, 1].set_title(f'{name}: Test\\n{config} - Accuracy: {test_acc:.2%}', fontsize=11)\n\nplt.suptitle('Confusion Matrices: Small vs. Medium vs. Large Networks', \n             fontsize=14, y=0.995)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Network Structure for Digits\n\nOur neural network:\n\n- Input layer: 64 inputs (one per pixel)\n- Hidden layer: 50 neurons with ReLU activation\n- Output layer: 10 neurons with softmax (one per digit)\n\n**Parameters:**\n\n- Hidden layer: 50 biases + 50×64 weights = 3,250 parameters\n- Output layer: 10 biases + 10×50 weights = 510 parameters\n- Total: 3,760 parameters to optimize"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Define ReLU Function"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def relu(z):\n    \"\"\"ReLU activation function: max(0, z)\"\"\"\n    return np.maximum(0, z)\n\ndef softmax(z):\n    \"\"\"Softmax activation: converts to probabilities\"\"\"\n    exp_z = np.exp(z - np.max(z))  # Subtract max for numerical stability\n    return exp_z / exp_z.sum()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Example: Forward Propagation with a Test Digit"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Take the first test example\nx_input = X_test[0]\ntrue_digit = y_test.iloc[0]\n\n# Get the trained weights and biases from our model\nW1 = nn_model.coefs_[0]  # Shape: (64, 50)\nb1 = nn_model.intercepts_[0]  # Shape: (50,)\nW2 = nn_model.coefs_[1]  # Shape: (50, 10)\nb2 = nn_model.intercepts_[1]  # Shape: (10,)\n\n# Hidden layer calculation\nz1 = x_input @ W1 + b1\nh = relu(z1)\n\n# Output layer calculation\nz2 = h @ W2 + b2\nprobs = softmax(z2)\n\npredicted_digit = np.argmax(probs)\n\n# Visualize the digit and predictions\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Show the digit\nax1.imshow(x_input.reshape(8, 8), cmap='gray_r')\nax1.axis('off')\nax1.set_title(f'True Digit: {true_digit}', fontsize=14)\n\n# Show the probabilities\nax2.bar(range(10), probs, color=['green' if i == predicted_digit else 'blue' for i in range(10)])\nax2.set_xlabel('Digit')\nax2.set_ylabel('Probability')\nax2.set_title(f'Predicted: {predicted_digit} (confidence: {probs[predicted_digit]:.1%})', fontsize=14)\nax2.set_xticks(range(10))\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nForward Propagation Details:\")\nprint(f\"  Input: 64 pixel values\")\nprint(f\"  Hidden layer: 50 neurons, max activation = {h.max():.3f}\")\nprint(f\"  Output layer: 10 probabilities\")\nprint(f\"  Prediction: {predicted_digit} (True: {true_digit})\")\nprint(f\"  Top 3 predictions:\")\nfor i in np.argsort(probs)[::-1][:3]:\n    print(f\"    Digit {i}: {probs[i]:.1%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n**Neural Networks for Digit Classification:**\n\n- Multi-layer perceptrons (MLPs) consist of neurons arranged in layers\n- Each neuron applies weights, bias, and activation function (ReLU for hidden, softmax for output)\n- Transform pixel inputs through layers to learn complex patterns\n- Parameters (weights and biases) are optimized during training via backpropagation\n\n**Key Findings from Complexity Analysis:**\n\n- Model complexity (number of parameters) affects both training and test accuracy\n- Larger networks can achieve higher training accuracy but may not always generalize better\n- The optimal architecture balances model capacity with generalization\n- Confusion matrices reveal which digits are most easily confused (e.g., 8 vs 3, 9 vs 4)\n\n**Best Practices:**\n\n- Normalize input features (pixel values) for better convergence\n- Use train/test split to evaluate generalization\n- Compare multiple architectures to find the best model\n- Monitor both training and test accuracy to detect overfitting\n- Use confusion matrices to identify specific classification challenges\n\n**Key Takeaways:**\n\n- Neural networks excel at image classification tasks\n- ReLU activation allows learning non-linear patterns\n- Softmax output gives interpretable probabilities\n- More neurons/layers increase capacity but also computational cost\n- Model selection requires balancing complexity with performance"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}